name: Performance Regression Detection

on:
  pull_request:
    branches: [ main, develop ]
    types: [opened, synchronize, reopened]
  push:
    branches: [ main ]
  schedule:
    # Weekly performance trend analysis
    - cron: '0 6 * * 1'

env:
  PERFORMANCE_THRESHOLD_PERCENT: 20  # Alert if performance degrades by more than 20%
  MEMORY_THRESHOLD_PERCENT: 15       # Alert if memory usage increases by more than 15%

jobs:
  # Performance baseline collection (main branch)
  collect-baseline:
    name: Collect Performance Baseline
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24.x'

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-1.24.x-${{ hashFiles('**/go.sum') }}

      - name: Setup LSP servers
        run: |
          go install golang.org/x/tools/gopls@latest
          pip install python-lsp-server[all]
          npm install -g typescript-language-server typescript

      - name: Build binary
        run: make local

      - name: Run performance benchmarks
        run: |
          export PATH=$PWD/bin:$PATH
          mkdir -p performance-results
          
          # Run benchmarks multiple times for statistical significance
          for i in {1..5}; do
            echo "=== Benchmark Run $i ===" >> performance-results/baseline-raw.txt
            go test -bench=. -benchmem -count=1 -timeout=30m ./tests/performance/... >> performance-results/baseline-raw.txt
            echo "" >> performance-results/baseline-raw.txt
          done

      - name: Process baseline results
        run: |
          # Extract and average benchmark results
          python3 -c "
          import re
          import json
          from statistics import mean
          
          # Parse benchmark results
          results = {}
          with open('performance-results/baseline-raw.txt', 'r') as f:
              content = f.read()
              
          # Extract benchmark data (BenchmarkName-N \t iterations ns/op MB/s B/op allocs/op)
          pattern = r'Benchmark(\w+)-\d+\s+(\d+)\s+(\d+)\s+ns/op\s+(\d+)\s+B/op\s+(\d+)\s+allocs/op'
          matches = re.findall(pattern, content)
          
          for match in matches:
              name, iterations, ns_per_op, bytes_per_op, allocs_per_op = match
              if name not in results:
                  results[name] = {'ns_per_op': [], 'bytes_per_op': [], 'allocs_per_op': []}
              results[name]['ns_per_op'].append(int(ns_per_op))
              results[name]['bytes_per_op'].append(int(bytes_per_op))
              results[name]['allocs_per_op'].append(int(allocs_per_op))
          
          # Calculate averages
          baseline = {}
          for name, data in results.items():
              baseline[name] = {
                  'ns_per_op': mean(data['ns_per_op']),
                  'bytes_per_op': mean(data['bytes_per_op']),
                  'allocs_per_op': mean(data['allocs_per_op']),
                  'runs': len(data['ns_per_op'])
              }
          
          with open('performance-results/baseline.json', 'w') as f:
              json.dump({
                  'commit': '${{ github.sha }}',
                  'timestamp': '${{ github.event.head_commit.timestamp }}',
                  'benchmarks': baseline
              }, f, indent=2)
          "

      - name: Upload baseline results
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline-${{ github.sha }}
          path: performance-results/
          retention-days: 90

      - name: Store baseline in cache
        uses: actions/cache/save@v4
        with:
          path: performance-results/baseline.json
          key: performance-baseline-main-${{ github.sha }}

  # Performance regression check (PRs)
  regression-check:
    name: Performance Regression Check
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24.x'

      - name: Cache Go modules
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/go-build
            ~/go/pkg/mod
          key: ${{ runner.os }}-go-1.24.x-${{ hashFiles('**/go.sum') }}

      - name: Setup LSP servers
        run: |
          go install golang.org/x/tools/gopls@latest
          pip install python-lsp-server[all]
          npm install -g typescript-language-server typescript

      - name: Build PR binary
        run: make local

      - name: Run PR performance benchmarks
        run: |
          export PATH=$PWD/bin:$PATH
          mkdir -p performance-results
          
          # Run benchmarks multiple times for statistical significance
          for i in {1..3}; do
            echo "=== PR Benchmark Run $i ===" >> performance-results/pr-raw.txt
            go test -bench=. -benchmem -count=1 -timeout=30m ./tests/performance/... >> performance-results/pr-raw.txt
            echo "" >> performance-results/pr-raw.txt
          done

      - name: Process PR results
        run: |
          # Extract and average PR benchmark results
          python3 -c "
          import re
          import json
          from statistics import mean
          
          # Parse benchmark results
          results = {}
          with open('performance-results/pr-raw.txt', 'r') as f:
              content = f.read()
              
          pattern = r'Benchmark(\w+)-\d+\s+(\d+)\s+(\d+)\s+ns/op\s+(\d+)\s+B/op\s+(\d+)\s+allocs/op'
          matches = re.findall(pattern, content)
          
          for match in matches:
              name, iterations, ns_per_op, bytes_per_op, allocs_per_op = match
              if name not in results:
                  results[name] = {'ns_per_op': [], 'bytes_per_op': [], 'allocs_per_op': []}
              results[name]['ns_per_op'].append(int(ns_per_op))
              results[name]['bytes_per_op'].append(int(bytes_per_op))
              results[name]['allocs_per_op'].append(int(allocs_per_op))
          
          # Calculate averages
          pr_results = {}
          for name, data in results.items():
              pr_results[name] = {
                  'ns_per_op': mean(data['ns_per_op']),
                  'bytes_per_op': mean(data['bytes_per_op']),
                  'allocs_per_op': mean(data['allocs_per_op']),
                  'runs': len(data['ns_per_op'])
              }
          
          with open('performance-results/pr-results.json', 'w') as f:
              json.dump({
                  'commit': '${{ github.event.pull_request.head.sha }}',
                  'pr_number': ${{ github.event.number }},
                  'benchmarks': pr_results
              }, f, indent=2)
          "

      - name: Fetch latest main baseline
        run: |
          # Try to get the most recent baseline from main branch
          curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/${{ github.repository }}/actions/artifacts?name=performance-baseline-" \
            | jq -r '.artifacts | sort_by(.created_at) | reverse | .[0].archive_download_url' > baseline_url.txt
          
          if [ -s baseline_url.txt ] && [ "$(cat baseline_url.txt)" != "null" ]; then
            echo "Found baseline artifact, downloading..."
            curl -L -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
              "$(cat baseline_url.txt)" -o baseline.zip
            unzip -o baseline.zip -d performance-results/
          else
            echo "No baseline found, creating empty baseline"
            echo '{"benchmarks": {}}' > performance-results/baseline.json
          fi

      - name: Compare performance and detect regressions
        id: regression-analysis
        run: |
          python3 -c "
          import json
          import sys
          import os
          
          # Load baseline and PR results
          try:
              with open('performance-results/baseline.json', 'r') as f:
                  baseline = json.load(f)
              baseline_benchmarks = baseline.get('benchmarks', {})
          except:
              baseline_benchmarks = {}
              
          with open('performance-results/pr-results.json', 'r') as f:
              pr_results = json.load(f)
          pr_benchmarks = pr_results['benchmarks']
          
          # Performance thresholds
          performance_threshold = float(os.environ['PERFORMANCE_THRESHOLD_PERCENT']) / 100
          memory_threshold = float(os.environ['MEMORY_THRESHOLD_PERCENT']) / 100
          
          regressions = []
          improvements = []
          report_lines = []
          
          report_lines.append('# Performance Regression Analysis\\n')
          report_lines.append(f'**PR #{pr_results[\"pr_number\"]}** vs Main Branch\\n')
          
          if not baseline_benchmarks:
              report_lines.append('âš ï¸ No baseline data available for comparison\\n')
              print('No baseline available')
              sys.exit(0)
          
          # Compare benchmarks
          for bench_name, pr_stats in pr_benchmarks.items():
              if bench_name not in baseline_benchmarks:
                  report_lines.append(f'ðŸ“Š **{bench_name}** (New benchmark)\\n')
                  continue
                  
              baseline_stats = baseline_benchmarks[bench_name]
              
              # Calculate percentage changes
              time_change = (pr_stats['ns_per_op'] - baseline_stats['ns_per_op']) / baseline_stats['ns_per_op']
              memory_change = (pr_stats['bytes_per_op'] - baseline_stats['bytes_per_op']) / baseline_stats['bytes_per_op']
              
              # Format changes
              time_change_str = f'{time_change*100:+.1f}%'
              memory_change_str = f'{memory_change*100:+.1f}%'
              
              # Determine status
              time_status = 'ðŸ”´' if time_change > performance_threshold else 'ðŸŸ¢' if time_change < -0.05 else 'ðŸŸ¡'
              memory_status = 'ðŸ”´' if memory_change > memory_threshold else 'ðŸŸ¢' if memory_change < -0.05 else 'ðŸŸ¡'
              
              report_lines.append(f'## {bench_name}\\n')
              report_lines.append(f'- **Execution Time**: {time_status} {time_change_str} ({pr_stats[\"ns_per_op\"]:.0f} vs {baseline_stats[\"ns_per_op\"]:.0f} ns/op)\\n')
              report_lines.append(f'- **Memory Usage**: {memory_status} {memory_change_str} ({pr_stats[\"bytes_per_op\"]} vs {baseline_stats[\"bytes_per_op\"]} B/op)\\n')
              report_lines.append(f'- **Allocations**: {pr_stats[\"allocs_per_op\"]} vs {baseline_stats[\"allocs_per_op\"]} allocs/op\\n\\n')
              
              # Track significant changes
              if time_change > performance_threshold or memory_change > memory_threshold:
                  regressions.append({
                      'benchmark': bench_name,
                      'time_change': time_change,
                      'memory_change': memory_change
                  })
              elif time_change < -0.1 or memory_change < -0.1:
                  improvements.append({
                      'benchmark': bench_name,
                      'time_change': time_change,
                      'memory_change': memory_change
                  })
          
          # Generate summary
          if regressions:
              report_lines.insert(2, f'âŒ **{len(regressions)} performance regression(s) detected**\\n\\n')
              for reg in regressions:
                  report_lines.insert(-1, f'- âš ï¸ **{reg[\"benchmark\"]}**: Time {reg[\"time_change\"]*100:+.1f}%, Memory {reg[\"memory_change\"]*100:+.1f}%\\n')
          elif improvements:
              report_lines.insert(2, f'âœ… **{len(improvements)} performance improvement(s) detected**\\n\\n')
          else:
              report_lines.insert(2, 'âœ… **No significant performance changes detected**\\n\\n')
          
          # Write report
          with open('performance-results/regression-report.md', 'w') as f:
              f.write(''.join(report_lines))
          
          # Set outputs
          has_regressions = len(regressions) > 0
          print(f'has_regressions={str(has_regressions).lower()}')
          
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'has_regressions={str(has_regressions).lower()}\\n')
              f.write(f'num_regressions={len(regressions)}\\n')
              f.write(f'num_improvements={len(improvements)}\\n')
          "

      - name: Comment PR with performance results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-results/regression-report.md', 'utf8');
            
            // Find existing performance comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const performanceComment = comments.data.find(comment => 
              comment.body.includes('Performance Regression Analysis')
            );
            
            if (performanceComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: performanceComment.id,
                body: report
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: report
              });
            }

      - name: Fail if regressions detected
        if: steps.regression-analysis.outputs.has_regressions == 'true'
        run: |
          echo "âŒ Performance regressions detected: ${{ steps.regression-analysis.outputs.num_regressions }}"
          echo "Review the performance analysis comment on this PR."
          exit 1

      - name: Upload performance comparison results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-comparison-pr-${{ github.event.number }}
          path: performance-results/
          retention-days: 30

  # Weekly performance trend analysis
  trend-analysis:
    name: Performance Trend Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
      - name: Checkout main branch
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24.x'

      - name: Fetch recent performance data
        run: |
          # Fetch performance baselines from the last 30 days
          curl -s -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/${{ github.repository }}/actions/artifacts?per_page=100" \
            | jq -r '.artifacts[] | select(.name | startswith("performance-baseline-")) | select(.created_at > (now - 30*24*3600 | todate)) | .archive_download_url' \
            > baseline_urls.txt

      - name: Generate trend report
        run: |
          mkdir -p trend-analysis
          
          python3 -c "
          import json
          import requests
          import zipfile
          import tempfile
          import os
          from datetime import datetime
          
          # Read baseline URLs
          with open('baseline_urls.txt', 'r') as f:
              urls = [line.strip() for line in f if line.strip()]
          
          print(f'Found {len(urls)} baseline artifacts')
          
          trend_data = []
          headers = {'Authorization': 'token ${{ secrets.GITHUB_TOKEN }}'}
          
          for url in urls:
              try:
                  response = requests.get(url, headers=headers)
                  with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp:
                      tmp.write(response.content)
                      tmp_path = tmp.name
                  
                  with zipfile.ZipFile(tmp_path, 'r') as zip_file:
                      if 'baseline.json' in zip_file.namelist():
                          with zip_file.open('baseline.json') as f:
                              data = json.load(f)
                              trend_data.append(data)
                  
                  os.unlink(tmp_path)
              except Exception as e:
                  print(f'Error processing {url}: {e}')
          
          # Sort by timestamp
          trend_data.sort(key=lambda x: x.get('timestamp', ''))
          
          # Generate trend report
          report = ['# Performance Trend Analysis (Last 30 Days)\\n\\n']
          
          if len(trend_data) < 2:
              report.append('âš ï¸ Insufficient data for trend analysis\\n')
          else:
              report.append(f'ðŸ“Š Analyzed {len(trend_data)} data points\\n\\n')
              
              # Analyze trends for each benchmark
              if trend_data:
                  latest = trend_data[-1]
                  oldest = trend_data[0]
                  
                  for bench_name in latest.get('benchmarks', {}):
                      if bench_name in oldest.get('benchmarks', {}):
                          latest_perf = latest['benchmarks'][bench_name]['ns_per_op']
                          oldest_perf = oldest['benchmarks'][bench_name]['ns_per_op']
                          
                          change = (latest_perf - oldest_perf) / oldest_perf * 100
                          
                          trend_emoji = 'ðŸ“ˆ' if change > 5 else 'ðŸ“‰' if change < -5 else 'âž¡ï¸'
                          report.append(f'## {bench_name} {trend_emoji}\\n')
                          report.append(f'- **30-day change**: {change:+.1f}%\\n')
                          report.append(f'- **Current**: {latest_perf:.0f} ns/op\\n')
                          report.append(f'- **30 days ago**: {oldest_perf:.0f} ns/op\\n\\n')
          
          with open('trend-analysis/trend-report.md', 'w') as f:
              f.write(''.join(report))
          
          print('Trend analysis completed')
          "

      - name: Create issue for trend analysis
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('trend-analysis/trend-report.md', 'utf8');
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Trend Analysis - ${new Date().toISOString().split('T')[0]}`,
              body: report,
              labels: ['performance', 'analysis', 'weekly-report']
            });

      - name: Upload trend analysis
        uses: actions/upload-artifact@v4
        with:
          name: performance-trend-analysis-${{ github.run_number }}
          path: trend-analysis/
          retention-days: 90